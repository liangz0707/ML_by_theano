{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Restricted Boltzmann Machines (RBM)\n",
    "\n",
    "首先需要具备的知识：\n",
    "- 玻尔兹曼机\n",
    "- 二部图\n",
    "- sigmoid函数\n",
    "- Bayes定理\n",
    "- 蒙塔卡罗方法\n",
    "- 马尔科夫链\n",
    "- MCMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Energy-Based Models (EBM) \n",
    "给予能量的模型就是通过一些属性，计算出一个标量的能量。我们通过学习将能量函数向我们希望的方向改进。例如根据我们的判断希望得到更低的能量。基于能量的模型就是通过能量方程定义一个概率分布模型：\n",
    "$$p(x)=\\frac{e^{-E(x)}}{Z}$$\n",
    "此时E为能量函数，p为概率模型，我们要让我们的结果计算出的概率最大（可以看出此时能量应该最小），又因为我们要用梯度下降法求解，所以我们可以给p加上负号-p来让-p最小\n",
    "Z是用来正规化的参数：\n",
    "$$Z=\\sum_x{e^{-E(x)}}$$\n",
    "一般能量函数可以通过随机梯度下降法优化（：the empirical negative log-likelihood of the training data实验负数log相似度）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在线性回归中softmax函数是一个例子\n",
    "$$E(x)= x_{max} - x（1）$$\n",
    "$$p(x) = \\frac{e^{x-x_{max}}}{\\sum_{x\\in D}{e^{x-x_{max}}}}（2）$$\n",
    "下面是梯度下降的过程，目的就是让 negativelog-likelihood最小,如下：\n",
    "$$\\iota(\\theta,D) = -\\frac{1}{N}\\sum{\\log{p(x,\\theta)}}  （3）$$\n",
    "\n",
    "为了让模型更有表现力，我们引入一些属性，但是这些属性是未知的我们用$h$表示，原本已知的属性还是用x表示：那么P写作：\n",
    "$$p(x) = \\sum_h{P(x,h)}=\\sum_h{\\frac{e^{-E(x,h)}}{Z}}$$\n",
    "为了和之前的内容保持一致性，作一下修改：\n",
    "我们另：$-F(x)$代替$-E(x)$:\n",
    "$$F(x)=-\\log{\\sum_h{e^{E(x,h)}}}$$\n",
    "则：\n",
    "$$p(x)=\\frac{e^{-F(x)}}{Z}$$\n",
    "\n",
    "$$Z=\\sum_x{e^{-F(x)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#上边给出了概率分布\n",
    "下面讨论如何让这个含有隐藏属性概率的最大\n",
    "\n",
    "首先我们的negative log-likelihood gradient如公式（3），这个时候需要通过梯度下降法求解，所以我们计算梯度(尝试推倒一下，很容易得到结果)：\n",
    "$$-\\frac{\\partial \\log{p(x)}}{\\partial \\theta} =\\frac{\\partial F(x)}{\\partial \\theta}-\\sum_x{p(x)\\frac{\\partial F(x)}{\\partial \\theta}}$$\n",
    "可以参考的网址：http://www.cnblogs.com/tornadomeet/archive/2013/03/27/2984725.html\n",
    "\n",
    "然而这个导数很难求，所以我们用蒙塔卡罗方法（统计模拟方法，使用随机数来解决计算问题）来进行简化。\n",
    "- 简化第一步：使用期望替代p(X),那么x就是服从p采样规律的\n",
    "现在的问题就是如何得到期望： Markov Chain Monte Carlo methods \n",
    "\n",
    "介绍波尔兹曼机（来源于百度百科）：\n",
    "\n",
    "玻尔兹曼机：BM是一种对称耦合的随机反馈型二值单元神经网络，由可见层和多个隐层组成，网络节点分为可见单元(visible unit)和隐单元(hidden unit)，用可见单元和隐单元来表达随机网络与随机环境的学习模型，通过权值表达单元之间的相关性\n",
    "\n",
    "受限玻尔兹曼机：RBM是一种玻尔兹曼机的变体，但限定模型必须为二分图。模型中包含对应输入参数的输入（可见）单元和对应训练结果的隐单元，图中的每条边必须连接一个可见单元和一个隐单元。（与此相对，“无限制”玻尔兹曼机包含隐单元间的边，使之成为递归神经网络。）这一限定使得相比一般玻尔兹曼机更高效的训练算法成为可能，特别是基于梯度的对比分歧（contrastivedivergence）算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#代码实现\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "try:\n",
    "    import PIL.Image as Image\n",
    "except ImportError:\n",
    "    import Image\n",
    "\n",
    "import numpy\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import os\n",
    "\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "\n",
    "from utils import tile_raster_images\n",
    "from mlp_lz import load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先引入功能包，下面定义类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RBM(object):\n",
    "    def __init(self,input,n_vsible=784,n_hidden=500,N=None,hbias=None,vBias=None,numpy_rng=None,theano_rng=None):\n",
    "        #\n",
    "        self.n_visible\n",
    "        self.n_hidden\n",
    "        \n",
    "        if numpy_rng is None:\n",
    "            numpy_rng = numpy.random.RandomState(1234)\n",
    "        #用于共哦共享的随机数\n",
    "        if theano_rng is None:\n",
    "            theano_rng = RandomStreams(numpy_rng.randint(2**30))\n",
    "        \n",
    "        if W is None:\n",
    "            initial_W = numpy.assary(\n",
    "                numpy_rng.uniform(\n",
    "                    low = -4 * numpy.sqrt(6./(n_hidden + n_visible)),\n",
    "                    high =4  * numpy.sqrt(6./(n_hidden + n_visible)),\n",
    "                ),\n",
    "                dtype = theano.config.floatX\n",
    "            )\n",
    "            W = theano.shared(value = initial_W , name = 'W' , brrow = True)\n",
    "        \n",
    "        if hbais is None:\n",
    "            hbias = theano.shared(\n",
    "                value = numpy.zeros(\n",
    "                    n_hidden,\n",
    "                    dtype = theano.config.floatX\n",
    "                ),\n",
    "                name = 'hbias',\n",
    "                borrow = True\n",
    "            )\n",
    "        if vbias is None:\n",
    "            # create shared variable for visible units bias\n",
    "            vbias = theano.shared(\n",
    "                value=numpy.zeros(\n",
    "                    n_visible,\n",
    "                    dtype=theano.config.floatX\n",
    "                ),\n",
    "                name='vbias',\n",
    "                borrow=True\n",
    "            )\n",
    "        #保存输入类型\n",
    "        self.input = input\n",
    "        if not input:\n",
    "            self.input = T.matrix('input')\n",
    "        \n",
    "        \n",
    "        self.W = W\n",
    "        self.hbias = hbias\n",
    "        self.vbias = vbias\n",
    "        self.theano_rng = theano_rng\n",
    "        # **** WARNING: It is not a good idea to put things in this list\n",
    "        # other than shared variables created in this function.\n",
    "        self.params = [self.W, self.hbias, self.vbias]\n",
    "        # end-snippet-1\n",
    "        \n",
    "        def free_energy(self , v_sample):\n",
    "            wx_b = T.dot(v_sample , self.W) + self.hbias\n",
    "            vbias_term = T.dot(v_sample,self.vbias)\n",
    "            hidden_term = T.sum(T.log(1+T.exp(wx_b)),axis = 1)\n",
    "            return -hidden_term - vbias_term\n",
    "        \n",
    "        #定义前向传播函数 #可视层进入隐藏层\n",
    "        def propup(self,vis):\n",
    "            pre_sigmoid_activation = T.dot(vis , self.W) + self.hbias\n",
    "            return [pre_sigmoid_activation , T.nnet.sigomid(pre_sigmoid_activation)]\n",
    "        #反向传播曾 从隐藏层计算可是曾\n",
    "        def propdown(self,hid):\n",
    "            pre_sigmoid_activation =  T.dot(hid , self.W.T) + self.vbias\n",
    "            return [pre_sigmoid_activation , T.nnet.sigomid(pre_sigmoid_activation)]\n",
    "        \n",
    "        #通过输入层计算激活层的状态（根据概率进行确定）\n",
    "        def sample_h_given_v(self,v0_sample):\n",
    "            pre_sigmoid_h1 , h1_mean = self.propup(v0_sample)\n",
    "            h1_sample = self.theano_rng.binomial(size = h1_mean.shape,n=1,p=h1_mean,\n",
    "                                                dtype=theano.config.floatX)\n",
    "            return [pre_sigmoid_h1,h1_mean,h1_sample]\n",
    "        \n",
    "        def sample_v_given_h(self,h0_sample):\n",
    "            pre_sigmoid_v1 , v1_mean = self.propdown(h0_sample)\n",
    "            v1_sample = self.theano_rng.binomial(size = v1_mean.shape,n=1,p=v1_mean,\n",
    "                                                dtype = theano.config.floatX)\n",
    "            return [pre_sigmoid_v1 ,v1_mean,v1_sample]\n",
    "        \n",
    "        #Gibbs采样 是用来使RBM（实际上就是一个函数）的分布逼近样本的分布？？？\n",
    "        #对RBM进行采样\n",
    "        def gibbs_hvh(self,h0_sample):\n",
    "            pre_sigmoid_v1,v1_mean,v1_sample = self.sample_v_given_h(h0_sample)\n",
    "            pre_sigmoid_h1,h1_mean,h1_sample = self.sample_h_given_v(v0_sample)\n",
    "            return [pre_sigmoid_v1,v1_mean,v1_sample,pre_sigmoid_h1,h1_mean,h1_sample]\n",
    "        \n",
    "        #改进CD\n",
    "        def gibbs_vhv(self,h0_sample):\n",
    "            pre_sigmoid_h1,h1_mean,h1_sample = self.sample_h_given_v(v0_sample)\n",
    "            pre_sigmoid_v1,v1_mean,v1_sample = self.sample_v_given_h(h0_sample)\n",
    "            return [pre_sigmoid_h1,h1_mean,h1_sample,pre_sigmoid_v1,v1_mean,v1_sample]\n",
    "        \n",
    "        \n",
    "        def get_cost_updates(self, lr=0.1, persistent=None, k=1):\n",
    "\n",
    "            # compute positive phase\n",
    "            pre_sigmoid_ph, ph_mean, ph_sample = self.sample_h_given_v(self.input)\n",
    "\n",
    "            # decide how to initialize persistent chain:\n",
    "            # for CD, we use the newly generate hidden sample\n",
    "            # for PCD, we initialize from the old state of the chain\n",
    "            if persistent is None:\n",
    "                chain_start = ph_sample\n",
    "            else:\n",
    "                chain_start = persistent\n",
    "       \n",
    "            (\n",
    "                [\n",
    "                    pre_sigmoid_nvs,\n",
    "                    nv_means,\n",
    "                    nv_samples,\n",
    "                    pre_sigmoid_nhs,\n",
    "                    nh_means,\n",
    "                    nh_samples\n",
    "                ],\n",
    "                updates\n",
    "            ) = theano.scan(\n",
    "                self.gibbs_hvh,\n",
    "                outputs_info=[None, None, None, None, None, chain_start],\n",
    "                n_steps=k\n",
    "            )\n",
    "          \n",
    "            # determine gradients on RBM parameters\n",
    "            # note that we only need the sample at the end of the chain\n",
    "            chain_end = nv_samples[-1]\n",
    "\n",
    "            cost = T.mean(self.free_energy(self.input)) - T.mean(\n",
    "                self.free_energy(chain_end))\n",
    "            # We must not compute the gradient through the gibbs sampling\n",
    "            gparams = T.grad(cost, self.params, consider_constant=[chain_end])\n",
    "            # end-snippet-3 start-snippet-4\n",
    "            # constructs the update dictionary\n",
    "            for gparam, param in zip(gparams, self.params):\n",
    "                # make sure that the learning rate is of the right dtype\n",
    "                updates[param] = param - gparam * T.cast(\n",
    "                    lr,\n",
    "                    dtype=theano.config.floatX\n",
    "                )\n",
    "            if persistent:\n",
    "                # Note that this works only if persistent is a shared variable\n",
    "                updates[persistent] = nh_samples[-1]\n",
    "                # pseudo-likelihood is a better proxy for PCD\n",
    "                monitoring_cost = self.get_pseudo_likelihood_cost(updates)\n",
    "            else:\n",
    "                # reconstruction cross-entropy is a better proxy for CD\n",
    "                monitoring_cost = self.get_reconstruction_cost(updates,\n",
    "                                                               pre_sigmoid_nvs[-1])\n",
    "\n",
    "            return monitoring_cost, updates\n",
    "            # end-snippet-4\n",
    "\n",
    "            \n",
    "#参数解释：学习速率，训练迭代次数，数据集，每次训练规模，          \n",
    "def test_rbm(learning_rate=0.1, training_epochs=15,\n",
    "             dataset='mnist.pkl.gz', batch_size=20,\n",
    "             n_chains=20, n_samples=10, output_folder='rbm_plots',\n",
    "             n_hidden=500):\n",
    "    \n",
    "    datasets = load_data(dataset)\n",
    "\n",
    "    #只需要训练集和测试集\n",
    "    train_set_x, train_set_y = datasets[0]\n",
    "    test_set_x, test_set_y = datasets[2]\n",
    "\n",
    "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "    \n",
    "    #定位数据位置\n",
    "    index = T.lscalar()    # index to a [mini]batch\n",
    "    x = T.matrix('x')  # the data is presented as rasterized images\n",
    "\n",
    "    #随机数生成器\n",
    "    rng = numpy.random.RandomState(123)\n",
    "    theano_rng = RandomStreams(rng.randint(2 ** 30))\n",
    "\n",
    "    # initialize storage for the persistent chain (state = hidden\n",
    "    # layer of chain)\n",
    "    persistent_chain = theano.shared(numpy.zeros((batch_size, n_hidden),\n",
    "                                                 dtype=theano.config.floatX),\n",
    "                                     borrow=True)\n",
    "\n",
    "    # 构造RBM\n",
    "    rbm = RBM(input=x, n_visible=28 * 28,\n",
    "              n_hidden=n_hidden, numpy_rng=rng, theano_rng=theano_rng)\n",
    "\n",
    "    # get the cost and the gradient corresponding to one step of CD-15\n",
    "    cost, updates = rbm.get_cost_updates(lr=learning_rate,\n",
    "                                         persistent=persistent_chain, k=15)\n",
    "\n",
    "    #################################\n",
    "    #     Training the RBM          #\n",
    "    #################################\n",
    "    if not os.path.isdir(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    os.chdir(output_folder)\n",
    "\n",
    "    # start-snippet-5\n",
    "    # it is ok for a theano function to have no output\n",
    "    # the purpose of train_rbm is solely to update the RBM parameters\n",
    "    train_rbm = theano.function(\n",
    "        [index],\n",
    "        cost,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            x: train_set_x[index * batch_size: (index + 1) * batch_size]\n",
    "        },\n",
    "        name='train_rbm'\n",
    "    )\n",
    "\n",
    "    plotting_time = 0.\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    # go through training epochs\n",
    "    for epoch in xrange(training_epochs):\n",
    "\n",
    "        # go through the training set\n",
    "        mean_cost = []\n",
    "        for batch_index in xrange(n_train_batches):\n",
    "            mean_cost += [train_rbm(batch_index)]\n",
    "\n",
    "        print 'Training epoch %d, cost is ' % epoch, numpy.mean(mean_cost)\n",
    "\n",
    "        # Plot filters after each training epoch\n",
    "        plotting_start = timeit.default_timer()\n",
    "        # Construct image from the weight matrix\n",
    "        image = Image.fromarray(\n",
    "            tile_raster_images(\n",
    "                X=rbm.W.get_value(borrow=True).T,\n",
    "                img_shape=(28, 28),\n",
    "                tile_shape=(10, 10),\n",
    "                tile_spacing=(1, 1)\n",
    "            )\n",
    "        )\n",
    "        image.save('filters_at_epoch_%i.png' % epoch)\n",
    "        plotting_stop = timeit.default_timer()\n",
    "        plotting_time += (plotting_stop - plotting_start)\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "\n",
    "    pretraining_time = (end_time - start_time) - plotting_time\n",
    "\n",
    "    print ('Training took %f minutes' % (pretraining_time / 60.))\n",
    "    # end-snippet-5 start-snippet-6\n",
    "    #################################\n",
    "    #     Sampling from the RBM     #\n",
    "    #################################\n",
    "    # find out the number of test samples\n",
    "    number_of_test_samples = test_set_x.get_value(borrow=True).shape[0]\n",
    "\n",
    "    # pick random test examples, with which to initialize the persistent chain\n",
    "    test_idx = rng.randint(number_of_test_samples - n_chains)\n",
    "    persistent_vis_chain = theano.shared(\n",
    "        numpy.asarray(\n",
    "            test_set_x.get_value(borrow=True)[test_idx:test_idx + n_chains],\n",
    "            dtype=theano.config.floatX\n",
    "        )\n",
    "    )\n",
    "    # end-snippet-6 start-snippet-7\n",
    "    plot_every = 1000\n",
    "    # define one step of Gibbs sampling (mf = mean-field) define a\n",
    "    # function that does `plot_every` steps before returning the\n",
    "    # sample for plotting\n",
    "    (\n",
    "        [\n",
    "            presig_hids,\n",
    "            hid_mfs,\n",
    "            hid_samples,\n",
    "            presig_vis,\n",
    "            vis_mfs,\n",
    "            vis_samples\n",
    "        ],\n",
    "        updates\n",
    "    ) = theano.scan(\n",
    "        rbm.gibbs_vhv,\n",
    "        outputs_info=[None, None, None, None, None, persistent_vis_chain],\n",
    "        n_steps=plot_every\n",
    "    )\n",
    "\n",
    "    # add to updates the shared variable that takes care of our persistent\n",
    "    # chain :.\n",
    "    updates.update({persistent_vis_chain: vis_samples[-1]})\n",
    "    # construct the function that implements our persistent chain.\n",
    "    # we generate the \"mean field\" activations for plotting and the actual\n",
    "    # samples for reinitializing the state of our persistent chain\n",
    "    sample_fn = theano.function(\n",
    "        [],\n",
    "        [\n",
    "            vis_mfs[-1],\n",
    "            vis_samples[-1]\n",
    "        ],\n",
    "        updates=updates,\n",
    "        name='sample_fn'\n",
    "    )\n",
    "\n",
    "    # create a space to store the image for plotting ( we need to leave\n",
    "    # room for the tile_spacing as well)\n",
    "    image_data = numpy.zeros(\n",
    "        (29 * n_samples + 1, 29 * n_chains - 1),\n",
    "        dtype='uint8'\n",
    "    )\n",
    "    for idx in xrange(n_samples):\n",
    "        # generate `plot_every` intermediate samples that we discard,\n",
    "        # because successive samples in the chain are too correlated\n",
    "        vis_mf, vis_sample = sample_fn()\n",
    "        print ' ... plotting sample ', idx\n",
    "        image_data[29 * idx:29 * idx + 28, :] = tile_raster_images(\n",
    "            X=vis_mf,\n",
    "            img_shape=(28, 28),\n",
    "            tile_shape=(1, n_chains),\n",
    "            tile_spacing=(1, 1)\n",
    "        )\n",
    "\n",
    "    # construct image\n",
    "    image = Image.fromarray(image_data)\n",
    "    image.save('samples.png')\n",
    "    # end-snippet-7\n",
    "    os.chdir('../')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
