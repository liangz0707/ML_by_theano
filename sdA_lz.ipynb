{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Stacked Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dA 可以构成深度网络，通过将前一层的编码（输出）作为输入传入当前层。**实际上就是一层连接一层**。对于这样的一个结构，首先进行的是预训练，就是对于每一层进行分别训练。和训练一层dA是一样的。第k层训练结束，那么在训练第k+1层。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当所有的层都训练结束之后，那么就要进行fine-tuning(微调、订正）\n",
    "这个过程是有监督的过程。目的是尽可能的减少预测误差。\n",
    "所以为了进行预测，需要在顶部加一个逻辑回归层（logistic regression）。\n",
    "之后我们训练这个网络就像训练一个深度感知网络一样。在这个过程当中，我们只修改dA部分的编码过程。这个过程是有监督的。所以我们就可以使用多层感知网络的方式来训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个sdA有两个角度，一方面是一串dA连接，另一方面是一个多层感知网。（并不是两个部分，而是即使一串dA又是，mlp他们共享参数，首先把它当作一串dA训练初始的参数，然后当作mlp来进行finetuning）\n",
    "在预训练的过程中我们之后只是用第一部分，逐层训练每一层的dA；\n",
    "在第二部分，我们训练mlp；\n",
    "这两部分是相关的：\n",
    "\n",
    "\n",
    "      1 dA层和mlp的sigmoid层共享参数\n",
    "      2 dA层的输出会作为mlp部分的中间层来进行计算，并且这个输出回馈给dA层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "\n",
    "import numpy\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "\n",
    "from mlp_lz import LogisticRegression,HiddenLayer, load_data\n",
    "from dA_lz import dA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面引入了一系列需要用到的库;\n",
    "下面就是定义一个SdA的类，用来生成这个网络的构造"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SdA(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        numpy_rng,\n",
    "        theano_rng=None,\n",
    "        n_ins=784,\n",
    "        hidden_layers_sizes=[500, 500],\n",
    "        n_outs=10,\n",
    "        corruption_levels=[0.1, 0.1]\n",
    "    ):\n",
    "        #这里有一个sigmoid层，之前没有提到，每两个dA之间需要一个sigmoid层\n",
    "        self.sigmoid_layers = []\n",
    "        #每层的dA\n",
    "        self.dA_layers = []\n",
    "        #记录每层的W和p\n",
    "        self.params = []\n",
    "        self.n_layers = len(hidden_layers_sizes)\n",
    "\n",
    "        assert self.n_layers > 0\n",
    "\n",
    "        if not theano_rng:\n",
    "            theano_rng = RandomStreams(numpy_rng.randint(2 ** 30))\n",
    "        # allocate symbolic variables for the data\n",
    "        self.x = T.matrix('x')  # the data is presented as rasterized images\n",
    "        self.y = T.ivector('y')  # the labels are presented as 1D vector of\n",
    "                                 # [int] labels\n",
    "        # end-snippet-1\n",
    "\n",
    "        # start-snippet-2\n",
    "        #进行逐层构造\n",
    "        for i in xrange(self.n_layers):\n",
    "            \n",
    "            #确定输入尺寸\n",
    "            if i == 0:\n",
    "                input_size = n_ins\n",
    "            else:\n",
    "                input_size = hidden_layers_sizes[i - 1]\n",
    "\n",
    "            #确定输出尺寸\n",
    "            if i == 0:\n",
    "                layer_input = self.x\n",
    "            else:\n",
    "                layer_input = self.sigmoid_layers[-1].output\n",
    "            #隐藏层：sigmoid层\n",
    "            sigmoid_layer = HiddenLayer(rng=numpy_rng,\n",
    "                                        input=layer_input,\n",
    "                                        n_in=input_size,\n",
    "                                        n_out=hidden_layers_sizes[i],\n",
    "                                        activation=T.nnet.sigmoid)\n",
    "            #加入列表\n",
    "            self.sigmoid_layers.append(sigmoid_layer)\n",
    "            #记录参数\n",
    "            self.params.extend(sigmoid_layer.params)\n",
    "\n",
    "            ##构造dA层，和sigmoid层共享参数\n",
    "            dA_layer = dA(numpy_rng=numpy_rng,\n",
    "                          theano_rng=theano_rng,\n",
    "                          input=layer_input,\n",
    "                          n_visible=input_size,\n",
    "                          n_hidden=hidden_layers_sizes[i],\n",
    "                          W=sigmoid_layer.W,\n",
    "                          bhid=sigmoid_layer.b)\n",
    "            self.dA_layers.append(dA_layer)\n",
    "        #作为多层感知网络，还需要再加一层逻辑回归层\n",
    "        self.logLayer = LogisticRegression(\n",
    "            input=self.sigmoid_layers[-1].output,\n",
    "            n_in=hidden_layers_sizes[-1],\n",
    "            n_out=n_outs\n",
    "        )\n",
    "        #extend就是往列表里填充另一个列表\n",
    "        #这里就是把参数都添加到sdA的params里面。\n",
    "        self.params.extend(self.logLayer.params)\n",
    "        #计算代价函数\n",
    "        self.finetune_cost = self.logLayer.negative_log_likelihood(self.y)\n",
    "        #计算误差\n",
    "        self.errors = self.logLayer.errors(self.y)\n",
    "    #逐个dA层进行预训练。只需要训练集\n",
    "    def pretraining_functions(self, train_set_x, batch_size):\n",
    "        # index to a [mini]batch\n",
    "        index = T.lscalar('index')  # index to a minibatch\n",
    "        corruption_level = T.scalar('corruption')  # % of corruption to use\n",
    "        learning_rate = T.scalar('lr')  # learning rate to use\n",
    "        # begining of a batch, given `index`\n",
    "        batch_begin = index * batch_size\n",
    "        # ending of a batch given `index`\n",
    "        batch_end = batch_begin + batch_size\n",
    "\n",
    "        pretrain_fns = []\n",
    "        #逐层训练\n",
    "        for dA in self.dA_layers:\n",
    "            # get the cost and the updates list\n",
    "            cost, updates = dA.get_cost_updates(corruption_level,\n",
    "                                                learning_rate)\n",
    "            #每给定一个index就进行一次训练。参见dA的训练过程\n",
    "            fn = theano.function(\n",
    "                inputs=[\n",
    "                    index,\n",
    "                    theano.Param(corruption_level, default=0.2),\n",
    "                    theano.Param(learning_rate, default=0.1)\n",
    "                ],\n",
    "                outputs=cost,\n",
    "                updates=updates,\n",
    "                givens={\n",
    "                    self.x: train_set_x[batch_begin: batch_end]\n",
    "                }\n",
    "            )\n",
    "            # 记录训练的函数\n",
    "            pretrain_fns.append(fn)\n",
    "\n",
    "        return pretrain_fns\n",
    "    #进行fine-tuning学习\n",
    "    def build_finetune_functions(self, datasets, batch_size, learning_rate):\n",
    "        (train_set_x, train_set_y) = datasets[0]\n",
    "        (valid_set_x, valid_set_y) = datasets[1]\n",
    "        (test_set_x, test_set_y) = datasets[2]\n",
    "\n",
    "        # compute number of minibatches for training, validation and testing\n",
    "        n_valid_batches = valid_set_x.get_value(borrow=True).shape[0]\n",
    "        n_valid_batches /= batch_size\n",
    "        n_test_batches = test_set_x.get_value(borrow=True).shape[0]\n",
    "        n_test_batches /= batch_size\n",
    "\n",
    "        index = T.lscalar('index')  # index to a [mini]batch\n",
    "\n",
    "        # compute the gradients with respect to the model parameters\n",
    "        gparams = T.grad(self.finetune_cost, self.params)\n",
    "\n",
    "        # compute list of fine-tuning updates\n",
    "        updates = [\n",
    "            (param, param - gparam * learning_rate)\n",
    "            for param, gparam in zip(self.params, gparams)\n",
    "        ]\n",
    "\n",
    "        train_fn = theano.function(\n",
    "            inputs=[index],\n",
    "            outputs=self.finetune_cost,\n",
    "            updates=updates,\n",
    "            givens={\n",
    "                self.x: train_set_x[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ],\n",
    "                self.y: train_set_y[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ]\n",
    "            },\n",
    "            name='train'\n",
    "        )\n",
    "\n",
    "        test_score_i = theano.function(\n",
    "            [index],\n",
    "            self.errors,\n",
    "            givens={\n",
    "                self.x: test_set_x[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ],\n",
    "                self.y: test_set_y[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ]\n",
    "            },\n",
    "            name='test'\n",
    "        )\n",
    "\n",
    "        valid_score_i = theano.function(\n",
    "            [index],\n",
    "            self.errors,\n",
    "            givens={\n",
    "                self.x: valid_set_x[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ],\n",
    "                self.y: valid_set_y[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ]\n",
    "            },\n",
    "            name='valid'\n",
    "        )\n",
    "\n",
    "        # Create a function that scans the entire validation set\n",
    "        def valid_score():\n",
    "            return [valid_score_i(i) for i in xrange(n_valid_batches)]\n",
    "\n",
    "        # Create a function that scans the entire test set\n",
    "        def test_score():\n",
    "            return [test_score_i(i) for i in xrange(n_test_batches)]\n",
    "\n",
    "        return train_fn, valid_score, test_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_SdA(finetune_lr=0.1, pretraining_epochs=15,\n",
    "             pretrain_lr=0.001, training_epochs=1000,\n",
    "             dataset='mnist.pkl.gz', batch_size=1):\n",
    "    datasets = load_data(dataset)\n",
    "\n",
    "    train_set_x, train_set_y = datasets[0]\n",
    "    valid_set_x, valid_set_y = datasets[1]\n",
    "    test_set_x, test_set_y = datasets[2]\n",
    "\n",
    "    # compute number of minibatches for training, validation and testing\n",
    "    n_train_batches = train_set_x.get_value(borrow=True).shape[0]\n",
    "    n_train_batches /= batch_size\n",
    "\n",
    "    # numpy random generator\n",
    "    # start-snippet-3\n",
    "    numpy_rng = numpy.random.RandomState(89677)\n",
    "    print '... building the model'\n",
    "    # construct the stacked denoising autoencoder class\n",
    "    sda = SdA(\n",
    "        numpy_rng=numpy_rng,\n",
    "        n_ins=28 * 28,\n",
    "        hidden_layers_sizes=[1000, 1000, 1000],\n",
    "        n_outs=10\n",
    "    )\n",
    "    # end-snippet-3 start-snippet-4\n",
    "    #########################\n",
    "    # PRETRAINING THE MODEL #\n",
    "    #########################\n",
    "    print '... getting the pretraining functions'\n",
    "    pretraining_fns = sda.pretraining_functions(train_set_x=train_set_x,\n",
    "                                                batch_size=batch_size)\n",
    "\n",
    "    print '... pre-training the model'\n",
    "    start_time = timeit.default_timer()\n",
    "    ## Pre-train layer-wise\n",
    "    corruption_levels = [.1, .2, .3]\n",
    "    for i in xrange(sda.n_layers):\n",
    "        # go through pretraining epochs\n",
    "        for epoch in xrange(pretraining_epochs):\n",
    "            # go through the training set\n",
    "            c = []\n",
    "            for batch_index in xrange(n_train_batches):\n",
    "                c.append(pretraining_fns[i](index=batch_index,\n",
    "                         corruption=corruption_levels[i],\n",
    "                         lr=pretrain_lr))\n",
    "            print 'Pre-training layer %i, epoch %d, cost ' % (i, epoch),\n",
    "            print numpy.mean(c)\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "\n",
    "    print >> sys.stderr, ('The pretraining code for file ' +\n",
    "                          os.path.split(__file__)[1] +\n",
    "                          ' ran for %.2fm' % ((end_time - start_time) / 60.))\n",
    "    # end-snippet-4\n",
    "    ########################\n",
    "    # FINETUNING THE MODEL #\n",
    "    ########################\n",
    "\n",
    "    # get the training, validation and testing function for the model\n",
    "    print '... getting the finetuning functions'\n",
    "    train_fn, validate_model, test_model = sda.build_finetune_functions(\n",
    "        datasets=datasets,\n",
    "        batch_size=batch_size,\n",
    "        learning_rate=finetune_lr\n",
    "    )\n",
    "\n",
    "    print '... finetunning the model'\n",
    "    # early-stopping parameters\n",
    "    patience = 10 * n_train_batches  # look as this many examples regardless\n",
    "    patience_increase = 2.  # wait this much longer when a new best is\n",
    "                            # found\n",
    "    improvement_threshold = 0.995  # a relative improvement of this much is\n",
    "                                   # considered significant\n",
    "    validation_frequency = min(n_train_batches, patience / 2)\n",
    "                                  # go through this many\n",
    "                                  # minibatche before checking the network\n",
    "                                  # on the validation set; in this case we\n",
    "                                  # check every epoch\n",
    "\n",
    "    best_validation_loss = numpy.inf\n",
    "    test_score = 0.\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    done_looping = False\n",
    "    epoch = 0\n",
    "\n",
    "    while (epoch < training_epochs) and (not done_looping):\n",
    "        epoch = epoch + 1\n",
    "        for minibatch_index in xrange(n_train_batches):\n",
    "            minibatch_avg_cost = train_fn(minibatch_index)\n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "\n",
    "            if (iter + 1) % validation_frequency == 0:\n",
    "                validation_losses = validate_model()\n",
    "                this_validation_loss = numpy.mean(validation_losses)\n",
    "                print('epoch %i, minibatch %i/%i, validation error %f %%' %\n",
    "                      (epoch, minibatch_index + 1, n_train_batches,\n",
    "                       this_validation_loss * 100.))\n",
    "\n",
    "                # if we got the best validation score until now\n",
    "                if this_validation_loss < best_validation_loss:\n",
    "\n",
    "                    #improve patience if loss improvement is good enough\n",
    "                    if (\n",
    "                        this_validation_loss < best_validation_loss *\n",
    "                        improvement_threshold\n",
    "                    ):\n",
    "                        patience = max(patience, iter * patience_increase)\n",
    "\n",
    "                    # save best validation score and iteration number\n",
    "                    best_validation_loss = this_validation_loss\n",
    "                    best_iter = iter\n",
    "\n",
    "                    # test it on the test set\n",
    "                    test_losses = test_model()\n",
    "                    test_score = numpy.mean(test_losses)\n",
    "                    print(('     epoch %i, minibatch %i/%i, test error of '\n",
    "                           'best model %f %%') %\n",
    "                          (epoch, minibatch_index + 1, n_train_batches,\n",
    "                           test_score * 100.))\n",
    "\n",
    "            if patience <= iter:\n",
    "                done_looping = True\n",
    "                break\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "    print(\n",
    "        (\n",
    "            'Optimization complete with best validation score of %f %%, '\n",
    "            'on iteration %i, '\n",
    "            'with test performance %f %%'\n",
    "        )\n",
    "        % (best_validation_loss * 100., best_iter + 1, test_score * 100.)\n",
    "    )\n",
    "    print >> sys.stderr, ('The training code for file ' +\n",
    "                          os.path.split(__file__)[1] +\n",
    "                          ' ran for %.2fm' % ((end_time - start_time) / 60.))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_SdA()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
